{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0cfc03-8a2c-45e2-a332-2a8329331c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:36:57.489896900Z",
     "start_time": "2024-04-24T23:36:54.328117800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x75db41bcdf30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from skimage import io\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4518876-3a3f-4d36-8aea-adba98c57cac",
   "metadata": {},
   "source": [
    "## Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb66b807-2c2b-48e4-81c1-eca1529eb652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:36:57.494855900Z",
     "start_time": "2024-04-24T23:36:57.492222100Z"
    }
   },
   "outputs": [],
   "source": [
    "class AkuDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"X\": self.X[idx],\n",
    "            \"y\": self.y[idx]\n",
    "        }\n",
    "        if self.transform:\n",
    "            sample[\"X\"] = self.transform(sample[\"X\"])\n",
    "\n",
    "        return sample\n",
    "\n",
    "def loadAkuDataset(labels_file, root_dir, data_split=(0.7, 0.2, 0.1), transform = None):\n",
    "    \"\"\"\n",
    "    @param labels_file: path to the csv file containing the labels for each image\n",
    "    @param root_dir: path to the dir containing the original and augmented image folders\n",
    "    @param data_split: three elements tuple containing the ratio of images for train, validation and test sets\n",
    "    @param transform: a concatenation of trasformations to apply to the images\n",
    "    @return a three element tuple containing the datasets for the training, validation and test sets\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # read the labels from the csv file\n",
    "    labels_df = pd.read_csv(labels_file)\n",
    "    labels_df.set_index(\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "    # reading the original images\n",
    "    original_dir = f\"{root_dir}/original\"\n",
    "    for filename in os.listdir(original_dir):\n",
    "        path = f\"{original_dir}/{filename}\"\n",
    "        X.append( Image.open(path) )\n",
    "        y.append(labels_df.loc[filename].values)\n",
    "\n",
    "    # reading the augmented images\n",
    "    X = X[:10]\n",
    "    y = y[:10]\n",
    "    \"\"\"\n",
    "    augmented_dir = f\"{root_dir}/augmented\"\n",
    "    for filename in os.listdir(augmented_dir):\n",
    "        path = f\"{augmented_dir}/{filename}\"\n",
    "        X.append( Image.open(path) )\n",
    "        y.append(labels_df.loc[filename].values)\n",
    "    \"\"\"\n",
    "    # split the data in train, validation and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.1, random_state=42)\n",
    "\n",
    "    return AkuDataset(X_train, y_train, transform), AkuDataset(X_val, y_val, transform), AkuDataset(X_test, y_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d918f1eb-3d18-4b18-b5f4-4851ad5814fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:36:57.689827100Z",
     "start_time": "2024-04-24T23:36:57.496863500Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_file = \"./data/overall_labels.csv\"\n",
    "root_dir = \"./data\"\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# creating the datasets\n",
    "train_dataset, val_dataset, test_dataset = loadAkuDataset(\n",
    "    labels_file, \n",
    "    root_dir, \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(400),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    ")\n",
    "\n",
    "# creating the data loaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb1113-7b55-4bc2-ac5f-0f476324386e",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "852751ae-9ae8-4922-ad42-bbb8cbe902ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:36:57.697347600Z",
     "start_time": "2024-04-24T23:36:57.691144Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs):\n",
    "    dataloaders = {\n",
    "        \"train\": train_dataloader,\n",
    "        \"val\": val_dataloader\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {\n",
    "        x: len(dataloaders[x].sampler)\n",
    "        for x in [\"train\", \"val\"]\n",
    "    }\n",
    "    \n",
    "    loss_train_list, loss_val_list = [], []\n",
    "    \n",
    "    # create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "            print(\"-\" * 10)\n",
    "            \n",
    "            for phase in [\"train\", \"val\"]: # for each epoch alternate between train and validation\n",
    "                if phase == \"train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                           \n",
    "                # iterate over data\n",
    "                for sample in dataloaders[phase]:\n",
    "                    inputs = sample[\"X\"]\n",
    "                    labels = sample[\"y\"]\n",
    "    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    # forward\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "                        if( phase == \"train\" ): # if training phase then backpropagate\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "        \n",
    "                    # compute statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    _ , top_labels = torch.max(labels, 1)\n",
    "                    running_corrects += torch.sum(torch.eq(preds, top_labels))\n",
    "                    #print(\"-\"*5)\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                if phase == 'train':\n",
    "                    loss_train_list.append((epoch, epoch_loss))\n",
    "                else:\n",
    "                    loss_val_list.append((epoch, epoch_loss))\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "    \n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model with best accuracy\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                    \n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model, loss_train_list, loss_val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ca4ac6f-5503-4153-b0f7-857676b00765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:38:56.197701200Z",
     "start_time": "2024-04-24T23:36:57.699920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bendico765/Scrivania/Gianluca/Università/CHL-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7009 Acc: 0.2857\n",
      "val Loss: 0.6759 Acc: 0.5000\n",
      "Best val Acc: 0.500000\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "LR = 0.001\n",
    "\n",
    "# downloading the original resnet network\n",
    "#model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model = nn.Sequential(nn.Conv2d(3,20,5), nn.ReLU(), nn.MaxPool2d(5), nn.Conv2d(20,64,5), nn.MaxPool2d(5), nn.ReLU(), nn.Flatten(), nn.Linear(5184, 1000), nn.Linear(1000, 100), nn.Linear(100, 2), nn.Softmax(dim=None))\n",
    "# setting resnet layers as untrainable\n",
    "\"\"\"\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# adding a fresh new final layer to finetune\n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 2), nn.Softmax(dim=None))\n",
    "\"\"\"\n",
    "model = model.to(device)\n",
    "\n",
    "# criterion to compute the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use sgd as optimizer\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr = LR)\n",
    "\n",
    "# train the model\n",
    "model, loss_train_list, loss_val_list = train_model(model, train_dataloader, val_dataloader, criterion, optimizer_ft, num_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4037b5-f9bb-4423-a179-e63d181328ab",
   "metadata": {},
   "source": [
    "### Visualizing some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d07886a-69a1-457f-b4c4-773bfba8037a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:38:56.201716Z",
     "start_time": "2024-04-24T23:38:56.195834700Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"\n",
    "        Display image for Tensor.\n",
    "        \n",
    "        @param inp: tensor representing an image    \n",
    "    \"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "def visualize_model(model, dataloader, class_names, num_images=6):\n",
    "    \"\"\"\n",
    "    Sample and plot a certain number of images from the dataset alongside\n",
    "    the labels predicted by the model.\n",
    "    \n",
    "    @param model: the model used to make predictions\n",
    "    @param dataloader: dataloader used to sample the images\n",
    "    @param class_names: name of the classes predicted by the model\n",
    "    @num_images: number of images shown\n",
    "    \"\"\"\n",
    "    # save the status of the model and set it to eval mode\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in dataloader: # iterate over batches of data\n",
    "            inputs = sample[\"X\"]\n",
    "            labels = sample[\"y\"]\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for  j in range(inputs.size()[0]): # unroll the batch\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                # check if we reached the number of images required\n",
    "                if images_so_far == num_images: \n",
    "                    model.train(mode=was_training) # reset the previous state of the model\n",
    "                    return\n",
    "                    \n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe88384-9521-4fe5-aabf-47245c99f9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:38:56.831598400Z",
     "start_time": "2024-04-24T23:38:56.200713Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEALTHY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAKU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mvisualize_model\u001b[0;34m(model, dataloader, class_names, num_images)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataloader: \u001b[38;5;66;03m# iterate over batches of data\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m         labels \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     39\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_model(model, val_dataloader, [\"HEALTHY\", \"AKU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30cdd2-aa35-4b12-9d0a-d04a4c473b0a",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf57b8b-ac03-4c37-98fa-389ebe4c7fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:38:59.971033700Z",
     "start_time": "2024-04-24T23:38:56.832600300Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = [] # labels predicted by the model\n",
    "y = [] # real labels\n",
    "\n",
    "# iterate over all the validation data\n",
    "for sample in val_dataloader:\n",
    "    inputs = sample[\"X\"]\n",
    "    labels = sample[\"y\"]\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    for _ in torch.max(outputs, 1).indices:\n",
    "        y_pred.append(_.item())\n",
    "\n",
    "    for _ in torch.max(labels, 1).indices:\n",
    "        y.append(_.item())\n",
    "        \n",
    "cm = confusion_matrix(y, y_pred, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"HEALTHY\", \"AKU\"])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d610d-16f8-417f-bf79-6af7190c6fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:39:00.063782600Z",
     "start_time": "2024-04-24T23:38:59.969996500Z"
    }
   },
   "outputs": [],
   "source": [
    "xs = [x for x, y in loss_train_list]\n",
    "ys = [y for x, y in loss_train_list]\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19368053368b8886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T23:39:00.149864800Z",
     "start_time": "2024-04-24T23:39:00.059766200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "xs = [x for x, y in loss_val_list]\n",
    "ys = [y for x, y in loss_val_list]\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128e356-1cc1-46ab-83e8-0a1cf659d2b2",
   "metadata": {},
   "source": [
    "### influence points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2430f170-b45f-4b2e-9236-2cc7e4d3b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_influence import BaseObjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdaed660-2def-48ac-a977-6e3e361b2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyObjective(BaseObjective):\n",
    "    def train_outputs(self, model, batch):\n",
    "        X = batch['X']\n",
    "        return model(X)\n",
    "\n",
    "    def train_loss_on_outputs(self, outputs, batch):\n",
    "        y = batch['y']\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        return F.cross_entropy(outputs, preds)  # mean reduction required\n",
    "\n",
    "    def train_regularization(self, params):\n",
    "        return 0\n",
    "\n",
    "    # training loss by default taken to be \n",
    "    # train_loss_on_outputs + train_regularization\n",
    "\n",
    "    def test_loss(self, model, params, batch):\n",
    "        X = batch['X']\n",
    "        y = batch['y']\n",
    "        outputs = model(X)\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        #preds = preds.float()\n",
    "        outputs = outputs.float()\n",
    "        \n",
    "        return F.cross_entropy(outputs, y)  # no regularization in test loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d767370d-c59b-4049-89cf-507511782097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_influence import AutogradInfluenceModule, CGInfluenceModule\n",
    "\n",
    "module = CGInfluenceModule(\n",
    "    model=model,\n",
    "    objective=MyObjective(),  \n",
    "    train_loader=train_dataloader,\n",
    "    test_loader=val_dataloader,\n",
    "    device='cpu',\n",
    "    damp=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ddbb2e-02df-4c1b-84e4-7e29cfc96062",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = module.influences([1, 2, 3], [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f184a72-35af-44a6-aa48-293bdc1f9a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
